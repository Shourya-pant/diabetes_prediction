{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07c5a8d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Diabetes Dataset Analysis\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Importing necessary libraries\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# Diabetes Dataset Analysis with Advanced Techniques\n",
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, PowerTransformer, RobustScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, precision_recall_curve, average_precision_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, StackingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "file_path = 'diabetes_dataset.csv'\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "except:\n",
    "    df = pd.read_csv(file_path, header=None)\n",
    "    column_names = ['Age', 'Sex', 'Ethnicity', 'BMI', 'Waist_Circumference',\n",
    "                    'Fasting_Blood_Glucose', 'HbA1c', 'Blood_Pressure_Systolic',\n",
    "                    'Blood_Pressure_Diastolic', 'Cholesterol_Total', 'Cholesterol_HDL',\n",
    "                    'Cholesterol_LDL', 'GGT', 'Serum_Urate', 'Physical_Activity_Level',\n",
    "                    'Dietary_Intake_Calories', 'Alcohol_Consumption', 'Smoking_Status',\n",
    "                    'Family_History_of_Diabetes', 'Previous_Gestational_Diabetes']\n",
    "    if df.iloc[0, 0] == 0:\n",
    "        df = df.iloc[1:].reset_index(drop=True)\n",
    "    df.columns = column_names\n",
    "\n",
    "# Enhanced Data Preprocessing\n",
    "def advanced_preprocess_data(df):\n",
    "    data = df.copy()\n",
    "    \n",
    "    # Handle categorical variables with advanced encoding\n",
    "    categorical_cols = ['Sex', 'Ethnicity', 'Physical_Activity_Level',\n",
    "                       'Alcohol_Consumption', 'Smoking_Status']\n",
    "    \n",
    "    # Create interaction features\n",
    "    data['BMI_Age'] = data['BMI'] * data['Age']\n",
    "    data['BMI_Waist_Ratio'] = data['BMI'] / data['Waist_Circumference']\n",
    "    data['Glucose_HbA1c_Ratio'] = data['Fasting_Blood_Glucose'] / data['HbA1c']\n",
    "    data['Cholesterol_Ratio'] = data['Cholesterol_Total'] / data['Cholesterol_HDL']\n",
    "    data['BP_Product'] = data['Blood_Pressure_Systolic'] * data['Blood_Pressure_Diastolic']\n",
    "    \n",
    "    # Create polynomial features for important numeric columns\n",
    "    numeric_cols = ['BMI', 'Fasting_Blood_Glucose', 'HbA1c', 'Waist_Circumference']\n",
    "    for col in numeric_cols:\n",
    "        data[f'{col}_Squared'] = data[col] ** 2\n",
    "    \n",
    "    # Advanced categorical encoding\n",
    "    for col in categorical_cols:\n",
    "        le = LabelEncoder()\n",
    "        data[col] = le.fit_transform(data[col])\n",
    "        # Add frequency encoding\n",
    "        data[f'{col}_Freq'] = data[col].map(data[col].value_counts(normalize=True))\n",
    "    \n",
    "    # Handle missing values with advanced techniques\n",
    "    for col in data.columns:\n",
    "        if data[col].isnull().sum() > 0:\n",
    "            if data[col].dtype == 'object':\n",
    "                data[col].fillna(data[col].mode()[0], inplace=True)\n",
    "            else:\n",
    "                # Use more robust imputation for numeric columns\n",
    "                median_val = data[col].median()\n",
    "                std_val = data[col].std()\n",
    "                data[col].fillna(data[col].median() + np.random.normal(0, std_val/4, size=data[col].isnull().sum()), inplace=True)\n",
    "    \n",
    "    # Add log transforms for skewed numeric columns\n",
    "    numeric_cols = data.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        if data[col].skew() > 1:\n",
    "            data[f'{col}_Log'] = np.log1p(data[col] - data[col].min() + 1)\n",
    "    \n",
    "    return data\n",
    "\n",
    "processed_df = advanced_preprocess_data(df)\n",
    "\n",
    "# Create enhanced target variable\n",
    "def create_advanced_target(df):\n",
    "    # Combine multiple risk factors for a more nuanced classification\n",
    "    high_risk = (\n",
    "        (df['HbA1c'] >= 6.5) |  # Standard diabetes threshold\n",
    "        (df['Fasting_Blood_Glucose'] >= 126) |  # Standard diabetes threshold\n",
    "        ((df['HbA1c'] >= 6.0) & (df['Fasting_Blood_Glucose'] >= 110) & \n",
    "         (df['BMI'] >= 30) & (df['Age'] >= 45)) |  # Combined risk factors\n",
    "        ((df['Family_History_of_Diabetes'] == 1) & (df['HbA1c'] >= 6.0))  # Genetic predisposition\n",
    "    ).astype(int)\n",
    "    return high_risk\n",
    "\n",
    "processed_df['Diabetes_Risk'] = create_advanced_target(processed_df)\n",
    "\n",
    "# Prepare data for modeling with advanced feature selection\n",
    "X = processed_df.drop(['Diabetes_Risk', 'HbA1c', 'Fasting_Blood_Glucose'], axis=1)\n",
    "y = processed_df['Diabetes_Risk']\n",
    "\n",
    "# Advanced feature scaling\n",
    "scaler = RobustScaler()  # More robust to outliers than StandardScaler\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "# Enhanced train-test split with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Advanced class balancing with SMOTETomek\n",
    "smote_tomek = SMOTETomek(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote_tomek.fit_resample(X_train, y_train)\n",
    "\n",
    "# Define optimized base models\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=20,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    max_features='sqrt',\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "gb_model = GradientBoostingClassifier(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    min_samples_split=5,\n",
    "    subsample=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "svm_model = SVC(\n",
    "    kernel='rbf',\n",
    "    C=10,\n",
    "    gamma='scale',\n",
    "    probability=True,\n",
    "    class_weight='balanced',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Advanced Neural Network\n",
    "def build_advanced_dl_model(input_dim):\n",
    "    model = Sequential([\n",
    "        Dense(256, activation='relu', input_shape=(input_dim,), kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),\n",
    "\n",
    "        Dense(128, activation='relu', kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),\n",
    "\n",
    "        Dense(64, activation='relu', kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        Dense(32, activation='relu', kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', tf.keras.metrics.AUC()]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Build and train stacking ensemble\n",
    "estimators = [\n",
    "    ('rf', rf_model),\n",
    "    ('gb', gb_model),\n",
    "    ('svm', svm_model)\n",
    "]\n",
    "\n",
    "stack = StackingClassifier(\n",
    "    estimators=estimators,\n",
    "    final_estimator=LogisticRegression(max_iter=1000),\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "# Train models\n",
    "print(\"Training Stacking Ensemble...\")\n",
    "stack.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Train Deep Learning model\n",
    "print(\"\\nTraining Deep Learning Model...\")\n",
    "dl_model = build_advanced_dl_model(X_train_balanced.shape[1])\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.2,\n",
    "    patience=5,\n",
    "    min_lr=0.00001\n",
    ")\n",
    "\n",
    "history = dl_model.fit(\n",
    "    X_train_balanced, y_train_balanced,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Make predictions\n",
    "stack_pred = stack.predict(X_test)\n",
    "dl_pred = (dl_model.predict(X_test) > 0.5).astype(int)\n",
    "\n",
    "# Create final ensemble prediction\n",
    "final_pred = np.round((stack_pred + dl_pred.reshape(-1)) / 2).astype(int)\n",
    "\n",
    "# Evaluate final model\n",
    "final_accuracy = accuracy_score(y_test, final_pred)\n",
    "final_auc = roc_auc_score(y_test, final_pred)\n",
    "\n",
    "print(\"\\nFinal Model Performance:\")\n",
    "print(f\"Accuracy: {final_accuracy:.4f}\")\n",
    "print(f\"AUC-ROC: {final_auc:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, final_pred))\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "cm = confusion_matrix(y_test, final_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix - Final Ensemble Model')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n",
    "\n",
    "# Save the best models\n",
    "print(\"\\nSaving models...\")\n",
    "joblib.dump(stack, 'stack_ensemble_model.pkl')\n",
    "dl_model.save('deep_learning_model.h5')\n",
    "\n",
    "print(\"\\nAnalysis complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3d_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
